{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here is an outline of the function flow with suggested DataFrame names for better clarity:\n",
    "\n",
    "1. **Main Function: main**\n",
    "   - **Reads:** Uploaded file (file)\n",
    "   - **Returns:** None\n",
    "   - **Writes:** None\n",
    "\n",
    "2. **File Reading Function: read_file**\n",
    "   - **Reads:** Uploaded file (file)\n",
    "   - **Returns:** Initial DataFrame (`initial_df`)\n",
    "   - **Writes:** None\n",
    "\n",
    "3. **Column Cleaning Function: clean_columns**\n",
    "   - **Reads:** Initial DataFrame (`initial_df`)\n",
    "   - **Returns:** Cleaned DataFrame (`cleaned_df`)\n",
    "   - **Writes:** None\n",
    "\n",
    "4. **Initial Data Inspection Function: display_initial_inspection**\n",
    "   - **Reads:** Cleaned DataFrame (`cleaned_df`)\n",
    "   - **Returns:** None\n",
    "   - **Writes:** None\n",
    "\n",
    "5. **Column Headers Display Function: display_column_headers**\n",
    "   - **Reads:** Cleaned DataFrame (`cleaned_df`)\n",
    "   - **Returns:** None\n",
    "   - **Writes:** None\n",
    "\n",
    "6. **Datatype Options Creation Function: create_datatype_options**\n",
    "   - **Reads:** Current column datatype (current_type)\n",
    "   - **Returns:** Datatype options (options)\n",
    "   - **Writes:** None\n",
    "\n",
    "7. **Datatype Conversion Function: convert_datatypes**\n",
    "   - **Reads:** Cleaned DataFrame (`cleaned_df`), Datatype map (datatype_map)\n",
    "   - **Returns:** Converted DataFrame (`converted_df`)\n",
    "   - **Writes:** None\n",
    "\n",
    "8. **Data Preview Function: preview_data**\n",
    "   - **Reads:** Converted DataFrame (`converted_df`), Datatype map (datatype_map)\n",
    "   - **Returns:** Sorted DataFrame (sorted_df)\n",
    "   - **Writes:** None\n",
    "\n",
    "9. **Sorted Dataset Saving Function: save_sorted_dataset**\n",
    "   - **Reads:** Sorted DataFrame (sorted_df), Filename (filename)\n",
    "   - **Returns:** None\n",
    "   - **Writes:** Sorted DataFrame to disk\n",
    "\n",
    "10. **Saved Dataset Loading Function: load_saved_dataset**\n",
    "    - **Reads:** None\n",
    "    - **Returns:** Loaded DataFrame (`loaded_df`)\n",
    "    - **Writes:** None\n",
    "\n",
    "11. **Trade Analysis Function: `analyze_trades`**\n",
    "    - **Reads:** Loaded DataFrame (`loaded_df`)\n",
    "    - **Returns:** None\n",
    "    - **Writes:** None\n",
    "\n",
    "Based on this outline, here are the suggested DataFrame names to be used throughout the program:\n",
    "\n",
    "- `initial_df` for the DataFrame read from the uploaded file.\n",
    "- `cleaned_df` for the DataFrame after cleaning column names.\n",
    "- `converted_df` for the DataFrame after datatype conversion.\n",
    "- `sorted_df` for the DataFrame after sorting.\n",
    "- `loaded_df` for the DataFrame loaded from a saved file.\n",
    "\n",
    "Let's update the code with these DataFrame names:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from itables import show, init_notebook_mode\n",
    "import nest_asyncio\n",
    "import pyarrow as pa\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "# Initialize iTables\n",
    "init_notebook_mode()\n",
    "\n",
    "# Set Streamlit to wide mode\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up logging\n",
    "log_file = Path(f\"trade_data_preparation_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_names(df):\n",
    "    return df.rename(columns=lambda x: x.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', ''))\n",
    "\n",
    "# Function to display initial data inspection\n",
    "def display_initial_inspection(cleaned_df):\n",
    "    st.subheader(\"Initial Data Inspection\")\n",
    "    st.write(\"First few rows of the uploaded file:\")\n",
    "    st.write(cleaned_df.head())\n",
    "\n",
    "# Function to display column headers and datatypes\n",
    "def display_column_headers(cleaned_df):\n",
    "    st.subheader(\"Extracted Column Headers and Datatypes\")\n",
    "    headers_df = pd.DataFrame({\n",
    "        'Column_Name': cleaned_df.columns,\n",
    "        'Column_Datatype': cleaned_df.dtypes\n",
    "    })\n",
    "    st.dataframe(headers_df)\n",
    "\n",
    "# Function to create datatype selection options\n",
    "def create_datatype_options(current_type):\n",
    "    options = []\n",
    "    if current_type == 'object':\n",
    "        options = ['int64', 'float64', 'datetime64', 'object']\n",
    "    elif current_type in ['int64', 'float64']:\n",
    "        options = ['int64', 'float64', 'object']\n",
    "    elif 'datetime64' in current_type:\n",
    "        options = ['datetime64', 'object']\n",
    "    else:\n",
    "        options = ['object']\n",
    "    return options\n",
    "\n",
    "# Function to convert datatypes\n",
    "def convert_datatypes(cleaned_df, datatype_map):\n",
    "    for col, new_type in datatype_map.items():\n",
    "        try:\n",
    "            if new_type == 'datetime64':\n",
    "                cleaned_df[col] = pd.to_datetime(cleaned_df[col], errors='coerce')\n",
    "            else:\n",
    "                cleaned_df[col] = cleaned_df[col].astype(new_type)\n",
    "            logging.info(f\"Converted column {col} to {new_type}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error converting column {col} to {new_type}: {str(e)}\")\n",
    "            st.error(f\"Error converting column {col} to {new_type}: {str(e)}\")\n",
    "    return cleaned_df\n",
    "\n",
    "# Function to preview data after datatype conversion\n",
    "def preview_data(converted_df, datatype_map):\n",
    "    st.subheader(\"Preview of Data After Datatype Conversion\")\n",
    "    st.write(\"You can sort the DataFrame by clicking on the column headers.\")\n",
    "    sorted_df = st.dataframe(converted_df)\n",
    "    st.write(converted_df.dtypes)\n",
    "    return sorted_df\n",
    "\n",
    "# Function to save sorted dataset\n",
    "def save_sorted_dataset(sorted_df, filename):\n",
    "    try:\n",
    "        sorted_df.to_csv(filename, index=False)\n",
    "        logging.info(f\"Successfully saved sorted dataset to {filename}\")\n",
    "        st.success(f\"Successfully saved sorted dataset to {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving sorted dataset to {filename}: {str(e)}\")\n",
    "        st.error(f\"Error saving sorted dataset to {filename}: {str(e)}\")\n",
    "\n",
    "# Function to load saved dataset\n",
    "def load_saved_dataset():\n",
    "    saved_files = [f for f in os.listdir() if f.startswith('trade_performance_dataset_cleaned_') and f.endswith('.xlsx')]\n",
    "    if saved_files:\n",
    "        selected_file = st.selectbox(\"Select a saved dataset\", saved_files)\n",
    "        try:\n",
    "            loaded_df = pd.read_excel(selected_file)\n",
    "            logging.info(f\"Loaded saved dataset from {selected_file}\")\n",
    "            st.success(f\"Loaded saved dataset from {selected_file}\")\n",
    "            return loaded_df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading dataset from {selected_file}: {str(e)}\")\n",
    "            st.error(f\"Error loading dataset from {selected_file}: {str(e)}\")\n",
    "    else:\n",
    "        st.info(\"No saved datasets found.\")\n",
    "        return None\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    st.title(\"Trade Transaction Performance Analyzer\")\n",
    "\n",
    "    # File upload\n",
    "    file = st.file_uploader(\"Upload your XLSX or CSV file\", type=[\"xlsx\", \"csv\"])\n",
    "    if not file:\n",
    "        st.stop()\n",
    "\n",
    "    # Read the file\n",
    "    @st.cache_data\n",
    "    def read_file(file):\n",
    "        try:\n",
    "            if file.name.endswith('.xlsx'):\n",
    "                return pd.read_excel(file)\n",
    "            else:\n",
    "                return pd.read_csv(file)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error reading file: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    initial_df = read_file(file)\n",
    "    if initial_df is None:\n",
    "        return\n",
    "\n",
    "    # Clean column names\n",
    "    @st.cache_data\n",
    "    def clean_columns(initial_df):\n",
    "        return clean_column_names(initial_df)\n",
    "\n",
    "    cleaned_df = clean_columns(initial_df)\n",
    "\n",
    "    # Display initial data inspection\n",
    "    display_initial_inspection(cleaned_df)\n",
    "\n",
    "    # Display column headers and datatypes\n",
    "    display_column_headers(cleaned_df)\n",
    "\n",
    "    # Create datatype selection options\n",
    "    datatype_options = {col: create_datatype_options(str(dtype)) for col, dtype in cleaned_df.dtypes.items()}\n",
    "\n",
    "    # Create interactive grid for datatype selection\n",
    "    with st.expander(\"Select Datatypes\"):\n",
    "        datatype_map = {}\n",
    "        for col, options in datatype_options.items():\n",
    "            current_type = str(cleaned_df[col].dtype)\n",
    "            if 'datetime64' in current_type:\n",
    "                current_type = 'datetime64'\n",
    "            datatype_map[col] = st.selectbox(f\"Select datatype for {col}\", options, index=options.index(current_type))\n",
    "\n",
    "    # Preview data after datatype conversion\n",
    "    if st.button(\"Preview Data After Datatype Conversion\"):\n",
    "        @st.cache_data\n",
    "        def convert_and_preview(cleaned_df, datatype_map):\n",
    "            converted_df = convert_datatypes(cleaned_df.copy(), datatype_map)\n",
    "            return converted_df\n",
    "\n",
    "        converted_df = convert_and_preview(cleaned_df, datatype_map)\n",
    "        sorted_df = preview_data(converted_df, datatype_map)\n",
    "\n",
    "        # Save sorted dataset\n",
    "        if st.button(\"Save Sorted Dataset\"):\n",
    "            current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            sorted_csv_filename = f\"trade_performance_dataset_sorted_{current_datetime}.csv\"\n",
    "            save_sorted_dataset(sorted_df, sorted_csv_filename)\n",
    "\n",
    "    # Save dataset\n",
    "    if st.button(\"Save Cleaned Dataset\"):\n",
    "        current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        csv_filename = f\"trade_performance_dataset_cleaned_{current_datetime}.csv\"\n",
    "        xlsx_filename = f\"trade_performance_dataset_cleaned_{current_datetime}.xlsx\"\n",
    "        \n",
    "        df_cleaned = convert_datatypes(cleaned_df, datatype_map)\n",
    "        save_sorted_dataset(df_cleaned, csv_filename)\n",
    "        df_cleaned.to_excel(xlsx_filename, index=False)\n",
    "        logging.info(f\"Successfully saved cleaned dataset to {csv_filename} and {xlsx_filename}\")\n",
    "        st.success(f\"Successfully saved cleaned dataset to {csv_filename} and {xlsx_filename}\")\n",
    "\n",
    "    # Load saved dataset\n",
    "    if st.button(\"Load Saved Dataset\"):\n",
    "        loaded_df = load_saved_dataset()\n",
    "        if loaded_df is not None:\n",
    "            display_initial_inspection(loaded_df)\n",
    "            display_column_headers(loaded_df)\n",
    "\n",
    "    # Analyze trades\n",
    "    if st.button(\"Analyze Trades\"):\n",
    "        if 'loaded_df' not in locals():\n",
    "            st.error(\"Please load or upload a dataset first.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            loaded_df['Cumulative_Profit_Loss'] = loaded_df['Profit_Loss'].cumsum()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error calculating Cumulative Profit/Loss: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        # Display data\n",
    "        st.subheader(\"Trade data:\")\n",
    "        st.dataframe(loaded_df)\n",
    "\n",
    "        # Create and display plot\n",
    "        st.subheader(\"Cumulative Profit/Loss Chart:\")\n",
    "        try:\n",
    "            fig = px.line(loaded_df, x='Opened', y='Cumulative_Profit_Loss', title='Cumulative Profit/Loss Over Time')\n",
    "            fig.update_xaxes(title='Date')\n",
    "            fig.update_yaxes(title='Cumulative Profit/Loss')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error creating plot: {str(e)}\")\n",
    "\n",
    "        # Allow user to select date range for chart\n",
    "        st.subheader(\"Select date range for chart:\")\n",
    "        try:\n",
    "            start_date = st.date_input(\"Start date\", value=loaded_df['Opened'].min())\n",
    "            end_date = st.date_input(\"End date\", value=loaded_df['Opened'].max())\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error selecting date range: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        # Filter data and create new plot\n",
    "        try:\n",
    "            filtered_df = loaded_df[(loaded_df['Opened'] >= start_date) & (loaded_df['Opened'] <= end_date)]\n",
    "            fig = px.line(filtered_df, x='Opened', y='Cumulative_Profit_Loss', title='Filtered Cumulative Profit/Loss Over Time')\n",
    "            fig.update_xaxes(title='Date')\n",
    "            fig.update_yaxes(title='Cumulative Profit/Loss')\n",
    "            st.subheader(\"Filtered Cumulative Profit/Loss Chart:\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error filtering data or creating filtered plot: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this updated code:\n",
    "- The DataFrame names are updated to `initial_df`, `cleaned_df`, `converted_df`, \n",
    "\n",
    "sorted_df\n",
    "\n",
    ", and `loaded_df` for better clarity.\n",
    "- The function signatures and calls are updated to use these new DataFrame names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
