{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Tools for Preparing Data for Analysis\n",
    "\n",
    "This project creates an interactive tools for cleaning and converting data for use in analysis tools.  \n",
    "\n",
    "In an automated system this would be implemented in a data \"ETL pipeline\" that makes many assumptions and checks on data as it is transformed from the input format to the needed analysis format(s). \n",
    "\n",
    "This project uses Streamlit and iTables, which both use the \"Arrow\" format for serializing data. \n",
    "Pandas uses its own internal (but standardized) datatype system).  \n",
    "\n",
    "This is explained below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error you're encountering is related to the serialization of a DataFrame to an Arrow table. This issue arises because Arrow has strict requirements for data types, and certain data types in your DataFrame are not compatible with Arrow's serialization process.\n",
    "\n",
    "### Explanation of the Error\n",
    "\n",
    "1. **Arrow Serialization**:\n",
    "   - Arrow requires specific data types for serialization. If a column contains mixed data types or unsupported data types, Arrow will fail to serialize the DataFrame.\n",
    "   - The error message indicates that the `Closed` column contains `datetime.datetime` objects, which Arrow expects to be in a specific format.\n",
    "   - The `Column_Datatype` column contains `int64` values, but Arrow did not recognize the Python value type when inferring an Arrow data type.\n",
    "\n",
    "2. **Pandas**:\n",
    "   - Pandas is flexible with data types and can handle mixed data types within a column. However, this flexibility can cause issues when interfacing with other libraries like Arrow that have stricter requirements.\n",
    "\n",
    "3. **Streamlit**:\n",
    "   - Streamlit uses Arrow for efficient data transfer between the server and the client. Therefore, DataFrames passed to Streamlit components must be Arrow-compatible.\n",
    "\n",
    "4. **iTables**:\n",
    "   - iTables is a library for interactive tables in Jupyter notebooks. It also relies on Arrow for efficient data handling and visualization.\n",
    "\n",
    "5. **pyarrow**:\n",
    "   - Pyarrow is the Python library for Apache Arrow. It provides tools for converting between Pandas DataFrames and Arrow tables, but it requires that the data types be compatible with Arrow.\n",
    "\n",
    "### Best Practices for Using These Packages Together\n",
    "\n",
    "1. **Ensure Data Type Compatibility**:\n",
    "   - Before passing a DataFrame to Streamlit or iTables, ensure that all columns have compatible data types.\n",
    "   - Convert `datetime` columns to Pandas `datetime64` format.\n",
    "   - Ensure that all columns have consistent data types (e.g., no mixed types within a column).\n",
    "\n",
    "2. **Use Explicit Data Type Conversions**:\n",
    "   - Explicitly convert columns to the appropriate data types using Pandas' `astype` method.\n",
    "   - For example, convert `datetime` columns using `pd.to_datetime`.\n",
    "\n",
    "3. **Handle Mixed Data Types**:\n",
    "   - If a column contains mixed data types, convert it to a single data type that is compatible with Arrow.\n",
    "   - For example, convert a column with mixed integers and strings to strings using `astype(str)`.\n",
    "\n",
    "4. **Check Data Types Before Serialization**:\n",
    "   - Use Pandas' `dtypes` attribute to check the data types of all columns before passing the DataFrame to Streamlit or iTables.\n",
    "   - Ensure that all data types are compatible with Arrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Updated Code with Data Type Compatibility Checks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "def clean_column_names(df):\n",
    "    return df.rename(columns=lambda x: x.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', ''))\n",
    "\n",
    "def analyze_trades(df, trade_col, opened_col, max_margin_col, profit_loss_col):\n",
    "    df['Cumulative_Profit_Loss'] = df[profit_loss_col].cumsum()\n",
    "    return df\n",
    "\n",
    "def create_plot(df, profit_loss_col):\n",
    "    fig = px.line(df, x='Opened', y='Cumulative_Profit_Loss', title='Cumulative Profit/Loss Over Time')\n",
    "    fig.update_xaxes(title='Date')\n",
    "    fig.update_yaxes(title='Cumulative Profit/Loss')\n",
    "    return fig\n",
    "\n",
    "def ensure_arrow_compatibility(df):\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "        elif pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = df[col].astype('int64')\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            df[col] = df[col].astype('float64')\n",
    "        elif pd.api.types.is_object_dtype(df[col]):\n",
    "            df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    st.title(\"Trade Transaction Performance Analyzer\")\n",
    "\n",
    "    # File upload\n",
    "    file = st.file_uploader(\"Upload your XLSX or CSV file\", type=[\"xlsx\", \"csv\"])\n",
    "    if not file:\n",
    "        st.stop()\n",
    "\n",
    "    # Read the file\n",
    "    if file.name.endswith('.xlsx'):\n",
    "        df = pd.read_excel(file)\n",
    "    else:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "    # Clean column names\n",
    "    df = clean_column_names(df)\n",
    "\n",
    "    # Ensure Arrow compatibility\n",
    "    df = ensure_arrow_compatibility(df)\n",
    "\n",
    "    # Display column names\n",
    "    st.subheader(\"Available columns:\")\n",
    "    st.write(df.columns.tolist())\n",
    "\n",
    "    # Get user input for column selection\n",
    "    st.subheader(\"Select columns for analysis:\")\n",
    "    trade_col = st.selectbox(\"Trade column\", df.columns)\n",
    "    opened_col = st.selectbox(\"Opened column\", df.columns)\n",
    "    max_margin_col = st.selectbox(\"Maximum Margin column\", df.columns)\n",
    "    profit_loss_col = st.selectbox(\"Profit/Loss column\", df.columns)\n",
    "\n",
    "    # Analyze trades\n",
    "    df = analyze_trades(df, trade_col, opened_col, max_margin_col, profit_loss_col)\n",
    "\n",
    "    # Sort by date\n",
    "    df = df.sort_values('Closed', ascending=False)\n",
    "\n",
    "    # Display data\n",
    "    st.subheader(\"Trade data:\")\n",
    "    st.dataframe(df)\n",
    "\n",
    "    # Create and display plot\n",
    "    st.subheader(\"Cumulative Profit/Loss Chart:\")\n",
    "    fig = create_plot(df, profit_loss_col)\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # Allow user to select date range for chart\n",
    "    st.subheader(\"Select date range for chart:\")\n",
    "    start_date = st.date_input(\"Start date\", value=df['Opened'].min())\n",
    "    end_date = st.date_input(\"End date\", value=df['Opened'].max())\n",
    "\n",
    "    # Filter data and create new plot\n",
    "    filtered_df = df[(df['Opened'] >= start_date) & (df['Opened'] <= end_date)]\n",
    "    fig = create_plot(filtered_df, profit_loss_col)\n",
    "    st.subheader(\"Filtered Cumulative Profit/Loss Chart:\")\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Ensure Arrow Compatibility**:\n",
    "   - The `ensure_arrow_compatibility` function converts columns to data types that are compatible with Arrow.\n",
    "   - `datetime` columns are converted to `datetime64`.\n",
    "   - Integer and float columns are converted to `int64` and `float64`, respectively.\n",
    "   - Object columns are converted to strings.\n",
    "\n",
    "2. **Use Explicit Data Type Conversions**:\n",
    "   - The `ensure_arrow_compatibility` function explicitly converts columns to the appropriate data types using Pandas' `astype` method.\n",
    "\n",
    "3. **Check Data Types Before Serialization**:\n",
    "   - The `ensure_arrow_compatibility` function checks the data types of all columns before passing the DataFrame to Streamlit.\n",
    "\n",
    "### Running the Code\n",
    "\n",
    "Run the updated code in your Streamlit application to ensure that the data types are correctly handled and the application runs without errors. The enhanced error-checking and handling will help identify where the problem lies and provide informative error messages to the user. If you encounter any specific errors, please provide the error message for further assistance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
